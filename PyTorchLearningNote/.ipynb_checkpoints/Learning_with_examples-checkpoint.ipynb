{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## two main features\n",
    " - an n-dimensional tensor\n",
    " - auto differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用numpy实现两层神经网络\n",
    "x --> z1 --> relu(z1) --> z2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本流程\n",
    " - 定义x,y,w1,w2\n",
    " - 构建迭代（以下操作均在迭代中）\n",
    "     - forward:计算y_pred\n",
    "     - loss:计算loss\n",
    "     - 计算梯度\n",
    "     - 更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:27439754.59211147\n",
      "epoch:100, loss:377.04451931385836\n",
      "epoch:200, loss:1.2946316517472183\n",
      "epoch:300, loss:0.007374963136117815\n",
      "epoch:400, loss:4.963957402777351e-05\n",
      "epoch:500, loss:3.578759414149514e-07\n",
      "epoch:600, loss:2.6760459674019172e-09\n",
      "epoch:700, loss:2.0504052634223703e-11\n",
      "epoch:800, loss:1.6019997531989256e-13\n",
      "epoch:900, loss:1.2737021935012425e-15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "N, H, D_in, D_out = 60, 100, 1000, 10\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H,D_out)\n",
    "\n",
    "epochs = 1000\n",
    "lr =1e-6\n",
    "\n",
    "for i in range(epochs):\n",
    "    y1 = x.dot(w1)\n",
    "    a1 = np.maximum(y1,0)\n",
    "    \n",
    "    y2 = a1.dot(w2)\n",
    "    loss = np.sum(np.square(y2- y))\n",
    "    if i%100 == 0:\n",
    "        print(\"epoch:{}, loss:{}\".format(i,loss))\n",
    "    \n",
    "    dy2 = 2*(y2 - y)\n",
    "    dw2 = a1.T.dot(dy2)\n",
    "    da1 = dy2.dot(w2.T)\n",
    "    da1_ = da1.copy()\n",
    "    da1_[y1<0] = 0\n",
    "    dw1 = x.T.dot(da1_)\n",
    "    \n",
    "    w1 = w1 - lr*dw1\n",
    "    w2 = w2 - lr*dw2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用torch的tensor实现\n",
    "- 和numpy的流程一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本流程\n",
    " - 定义x,y,w1,w2\n",
    " - 构建迭代（以下操作均在迭代中）\n",
    "     - forward:计算y_pred\n",
    "     - loss:计算loss\n",
    "     - 计算梯度\n",
    "     - 更新参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 几个函数\n",
    " - `torch.randn`\n",
    " - `x.mm()`\n",
    " - `h.clamp(min = 0)`\n",
    " - `h.clone()`\n",
    " - `y.pow(2)`\n",
    " - `loss.item()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31675672.0\n",
      "100 599.44287109375\n",
      "200 5.669238567352295\n",
      "300 0.09221171587705612\n",
      "400 0.002032718388363719\n",
      "500 0.00016421520558651537\n",
      "600 4.14446258218959e-05\n",
      "700 1.7756441593519412e-05\n",
      "800 1.0123154424945824e-05\n",
      "900 6.781424872315256e-06\n"
     ]
    }
   ],
   "source": [
    "N, H, D_in, D_out = 60, 100, 1000, 10\n",
    "x = torch.randn(N, D_in,  device = device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device = device, dtype = dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H,  device = device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device = device, dtype = dtype)\n",
    "\n",
    "lr = 1e-6\n",
    "epochs = 1000\n",
    "for t in range(epochs):\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min = 0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t%100 == 0:\n",
    "        print(t,loss)\n",
    "    \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    w1 = w1 - lr* grad_w1\n",
    "    w2 = w2 - lr * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 自动微分实现两层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"#0000dd\">自动微分引入后不用再手动计算梯度</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本流程\n",
    " - 定义x,y,w1,w2\n",
    " - 构建迭代（以下操作均在迭代中）\n",
    "     - forward:计算y_pred\n",
    "     - loss:`计算loss`变成`定义loss`\n",
    "     - `计算梯度`变为`loss.backword()`\n",
    "     - 更新参数 <font color = \"#dd0000\">更新之后参数梯度清零</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算图的说明\n",
    " - node --> tensor\n",
    " - edge --> function\n",
    " - `x.requires_grad = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30176984.0\n",
      "100 706.0363159179688\n",
      "200 10.640213966369629\n",
      "300 0.21382303535938263\n",
      "400 0.0048640393652021885\n",
      "500 0.0002953430521301925\n",
      "600 6.683335959678516e-05\n",
      "700 2.9312617698451504e-05\n",
      "800 1.709126081550494e-05\n",
      "900 1.1490310498629697e-05\n"
     ]
    }
   ],
   "source": [
    "N, H, D_in, D_out = 60, 100, 1000, 10\n",
    "x = torch.randn(N, D_in,  device = device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device = device, dtype = dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H,  device = device, dtype=dtype, requires_grad = True)\n",
    "w2 = torch.randn(H, D_out, device = device, dtype = dtype, requires_grad = True)\n",
    "\n",
    "\n",
    "lr = 1e-6\n",
    "epochs = 1000\n",
    "for t in range(epochs):\n",
    "    ## compute the y_pred directly\n",
    "    y_pred = x.mm(w1).clamp(min = 0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    ## loss.item get the scalar val in the loss\n",
    "    if t%100 == 0:\n",
    "        print(t,loss.item())\n",
    "        \n",
    "    ## backward\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w1 -= lr*w1.grad\n",
    "        w2 -= lr*w2.grad\n",
    "        \n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义微分函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"#0000dd\">自定义自动微分函数,代替上一个cell中的`clamp`函数</font><br/>\n",
    "\n",
    "#### 定义一个新函数时定义\n",
    " - forward: input --> output 类似于y = y(x)\n",
    " - backward: 上一步输出的grad --> 经过这个函数新的grad \n",
    " \n",
    "![定义新函数的自动微分时各个输入输出的关系](./pytorch_autograd_function.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本流程与使用自动微分（上一个cell）的方法一样\n",
    " - 定义x,y,w1,w2\n",
    " - 构建迭代（以下操作均在迭代中）\n",
    "     - forward:计算y_pred  <font color = \"#dd0000\">使用自定义的relu函数计算</font>\n",
    "     - loss:`定义loss`\n",
    "     - `loss.backword()`\n",
    "     - 更新参数 <font color = \"#dd0000\">更新之后参数梯度清零</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `ctx.save_for_backward`\n",
    " - `ctx.saved_tensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRelu(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        args: input\n",
    "        output: \n",
    "        \n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min = 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input<0] = 0\n",
    "        return grad_input\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28664512.0\n",
      "100 316.8251647949219\n",
      "200 0.9898596405982971\n",
      "300 0.005892891436815262\n",
      "400 0.00017391871369909495\n",
      "500 3.390698111616075e-05\n",
      "600 1.4100089174462482e-05\n",
      "700 8.332450306625105e-06\n",
      "800 5.6630774452059995e-06\n",
      "900 4.236172117089154e-06\n"
     ]
    }
   ],
   "source": [
    "N, H, D_in, D_out = 60, 100, 1000, 10\n",
    "x = torch.randn(N, D_in,  device = device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device = device, dtype = dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H,  device = device, dtype=dtype, requires_grad = True)\n",
    "w2 = torch.randn(H, D_out, device = device, dtype = dtype, requires_grad = True)\n",
    "\n",
    "\n",
    "lr = 1e-6\n",
    "epochs = 1000\n",
    "for t in range(epochs):\n",
    "    relu = CustomRelu.apply\n",
    "    ## compute the y_pred directly\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    ## loss.item get the scalar val in the loss\n",
    "    if t%100 == 0:\n",
    "        print(t,loss.item())\n",
    "        \n",
    "    ## backward\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w1 -= lr*w1.grad\n",
    "        w2 -= lr*w2.grad\n",
    "        \n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow 静态图\n",
    "- 动态的图：对不同的输入进行不同的计算\n",
    "- 静态的图：通过循环实现展开操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/topsy/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/topsy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/topsy/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/topsy/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/topsy/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24619446.0\n",
      "200.91222\n",
      "0.53667057\n",
      "0.0024911969\n",
      "8.858916e-05\n"
     ]
    }
   ],
   "source": [
    "N, H, D_in, D_out = 60, 100, 1000, 10\n",
    "x = tf.placeholder(tf.float32, shape = (None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape = (None,D_out))\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))\n",
    "\n",
    "h = tf.matmul(x,w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)\n",
    "\n",
    "loss = tf.reduce_sum((y - y_pred)**2.0)\n",
    "\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1,w2])\n",
    "## tf中参数更新是计算图的一部分\n",
    "lr = 1e-6\n",
    "new_w1 = w1.assign(w1 - lr*grad_w1)\n",
    "new_w2 = w2.assign(w2 - lr*grad_w2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x_value = np.random.randn(N, D_in)\n",
    "    y_value = np.random.randn(N, D_out)\n",
    "    \n",
    "    for t in range(500):\n",
    "        loss_value, _, _ =sess.run([loss, new_w1, new_w2], feed_dict = {x:x_value, y:y_value})\n",
    "        if t%100 == 0:\n",
    "            print(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用nn模块实现\n",
    " - 不用编写层间函数\n",
    " - 给定每一层输入与输出的维度即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本流程\n",
    " - 定义x,y\n",
    " - 定义模型的结构\n",
    " - 定义loss\n",
    " - 构建迭代（以下操作均在迭代中）\n",
    "     - forward:计算y_pred  <font color = \"#dd0000\">`model(x)`</font>\n",
    "     -  <font color = \"#dd0000\">backward之前模型梯度清零</font>\n",
    "     - `loss.backword()`\n",
    "     - 更新参数 <font color = \"#dd0000\">更新之后不必参数梯度清零</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.zero_grad()`在pytorch中，参数在backward中积累梯度，在backward前清零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 640.214599609375\n",
      "100 589.9837036132812\n",
      "200 546.4067993164062\n",
      "300 508.3091735839844\n",
      "400 474.4449157714844\n",
      "500 444.0259094238281\n",
      "600 416.2925720214844\n",
      "700 391.2334899902344\n",
      "800 368.28009033203125\n",
      "900 347.1427001953125\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "N, H, D_in, D_out = 60, 100, 1000, 10\n",
    "x = torch.randn(N, D_in,  device = device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device = device, dtype = dtype)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in,H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H,D_out),\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction = \"sum\")\n",
    "\n",
    "lr = 1e-6\n",
    "for t in range(1000):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t%100 == 0:\n",
    "        print(t,loss.item())\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= lr*param.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch 优化器\n",
    " - 自动更新参数 以Adam为例\n",
    " - `optimizer.setup()`来更新参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本流程\n",
    " - 定义x,y\n",
    " - 定义模型的结构\n",
    " - 定义loss\n",
    " - 定义优化器\n",
    " - 构建迭代（以下操作均在迭代中）\n",
    "     - forward:计算y_pred  <font color = \"#dd0000\">`model(x)`</font>\n",
    "     - loss:`定义loss`\n",
    "     -  <font color = \"#dd0000\">backward之前模型梯度清零</font>\n",
    "     - `loss.backword()`\n",
    "     - 更新参数: `optimizer.setup()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 603.3372192382812\n",
      "100 587.5303955078125\n",
      "200 572.2085571289062\n",
      "300 557.3739624023438\n",
      "400 542.9630737304688\n",
      "500 529.021484375\n",
      "600 515.567626953125\n",
      "700 502.46270751953125\n",
      "800 489.7728576660156\n",
      "900 477.42999267578125\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "N, H, D_in, D_out = 60, 100, 1000, 10\n",
    "x = torch.randn(N, D_in,  device = device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device = device, dtype = dtype)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in,H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H,D_out),\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction = \"sum\")\n",
    "\n",
    "lr = 1e-6\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "for t in range(1000):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t%100 == 0:\n",
    "        print(t,loss.item())\n",
    "            \n",
    "    #梯度清零\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    ## update parameters\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义nn模型\n",
    " - 通过创建`nn.Moudle`的子类实现新的模型结构\n",
    " - 模型结构在`forward`中构造"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自定义模型等同于`torch.nn.Sequential`\n",
    " - 初始化的时候构造基本层(`torch.nn.Linear`)的实例\n",
    " - 在`forward`中构造模型结构（端到端：x经过forward直接得到y）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self,D_in, H, D_out):\n",
    "        ## 定义基本层\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        构造模型的结构\n",
    "        args:\n",
    "            x:输入\n",
    "        output:\n",
    "            返回一个计算结果\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 667.10009765625\n",
      "100 3.46479868888855\n",
      "200 0.05651705712080002\n",
      "300 0.002222836948931217\n",
      "400 0.00012843955482821912\n",
      "500 9.304566447099205e-06\n",
      "600 7.719827976870874e-07\n",
      "700 7.197548512749563e-08\n",
      "800 9.258323885319442e-09\n",
      "900 2.3363508905305252e-09\n"
     ]
    }
   ],
   "source": [
    "N, H, D_in, D_out = 60, 100, 1000, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = TwoLayerNet(D_in, H,D_out)\n",
    "criterion = torch.nn.MSELoss(reduction= 'sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-4)\n",
    "for t in range(1000):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t%100 == 0:\n",
    "        print(t,loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 控制流(自定义) + 参数分享 \n",
    "-  中间层的使用次数是随机的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本流程\n",
    " - 将自定义的模型替换`torch.nn.Sequential`，其他与使用优化器的流程一致\n",
    "\n",
    " - 自定义模型（类）\n",
    "     - 初始化定义基本层\n",
    "     - forward定义模型结构\n",
    " - 定义x,y\n",
    " -  <font color = \"#dd0000\">实例化自定的模型类</font>\n",
    " - loss函数\n",
    " - 优化器的定义\n",
    " - 构建迭代（以下操作均在迭代中）\n",
    "     - forward:计算y_pred  <font color = \"#dd0000\">`model(x)`</font>\n",
    "     - loss:`定义loss`\n",
    "     -  <font color = \"#dd0000\">backward之前模型梯度清零</font>\n",
    "     - `loss.backword()`\n",
    "     - 更新参数 <font color = \"#dd0000\">更新之后不必参数梯度清零</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        # 构造三个用于向前传播的 nn.Linear 实例\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_layer = torch.nn.Linear(D_in,H)\n",
    "        self.middle_layer = torch.nn.Linear(H,H)\n",
    "        self.output_layer = torch.nn.Linear(H, D_out)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        ## 每一次forward都构造一个动态的计算图，（通过for 或者 if判断语句来控制\n",
    "        ## 随机选取0~3个中间层\n",
    "        h_relu = self.input_layer(x).clamp(min = 0)\n",
    "        for _ in range(random.randint(0,3)):\n",
    "            h_relu = self.middle_layer(h_relu).clamp(min = 0)\n",
    "        h_relu = self.output_layer(h_relu)\n",
    "        return h_relu\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 618.580810546875\n",
      "100 21.41787338256836\n",
      "200 6.948872089385986\n",
      "300 1.4741227626800537\n",
      "400 0.9266403913497925\n",
      "500 2.9530601501464844\n",
      "600 0.2710525393486023\n",
      "700 0.32364121079444885\n",
      "800 0.03936019167304039\n",
      "900 0.12787111103534698\n"
     ]
    }
   ],
   "source": [
    "N, H, D_in, D_out = 60, 100, 1000, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "##构建模型\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "##选择loss函数\n",
    "criterion = torch.nn.MSELoss(reduction = 'sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-4, momentum=0.9)\n",
    "for t in range(1000):\n",
    "    ##计算预测值\n",
    "    y_pred = model(x)\n",
    "    ## 计算loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t%100==0:\n",
    "        print(t, loss.item())\n",
    "    ##梯度清零    \n",
    "    optimizer.zero_grad()\n",
    "    ##向后传播\n",
    "    loss.backward()\n",
    "    ##更新参数\n",
    "    optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
